# 3.7梯度下降法

## 基本思想
在计算loss极致时一般采用求导法，但是在高维的时候倒数特别难求，就线随便给变量一个起始值，算出loss的倒数，若斜率>0，则给原式减去起始值乘学习率，再重复刚才过程，直到loss越接近0越好则求到一个极值。
![avatar](\梯度下降法.png)

## 批量梯度下降(Batch Gradient Descent)
### 损失函数对参数的偏导

### ![avatar](\偏导.png)

## 随机梯度下降(Stochastic Gradient Descent)
每次随机选择一个方向，每次迭代处理的数据很少，达到最小值不会稳定下来，会继续弹跳，因此算法停止时，最终的参数是一个比较好的参数，但不是最佳的参数
![avatar](\随机梯度下降.png)

## 学习率问题
### 学习率太小
很久都找不到极值
![avatar](\学习率太小.png)
### 学习率太大
步子太大，算法发现按，在各处跳跃，越过极值跳到对面，每一步上都里解决方案越来越远
![avatar](\学习率太大.png)

### 确定一个好的学习率
1. 网络搜索
   设定迭代次数，尝试几种可能的参数，比较结果，选择最优
2. 梯度限制（常用）
   设置大量迭代，当tidings向量变小于一个值时停止

### 局部最优与全局最优

### ![avatar](\局部最优.png)