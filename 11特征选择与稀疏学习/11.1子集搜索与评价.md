# 11.1子集搜索与评价
## 概念
### 特征feature
如西瓜的色泽、根蒂、敲声、纹理、触感等
### 相关特征relevant feature
对当前学习任务有用的属性
### 无关特征irrelevant
对当前学习任务无有用的属性
### 冗余特征redudant feature
* 如立方体，长宽高底面积
* 可以由其他特征推演出来
* 有时有用，有事没用
### 特征选择feature selection
* 从给定特征几何中选择出相关特征子集的过程
* 特征选择是一个“数据预处理”的过程
## 本章假设
* 数据集不设计冗余特征
* 初始特征集包含了所有重要信息
## 特征选择思路
若没有任何邻域知识作为先验假设:
1. 遍历所有可能子集
  * 计算上不可行
  * 组合爆炸
2. 迭代
   * 确定候选自己
   * 评价
   * 针对评价结果产生下一个子集
   * 直到无法找到更好的候选子集为止
## 子集搜索与评价的目的
从特征集合中选取一个包含了多有重要信息特征的子集
## 子集搜索
### 前向搜索forward
**单特征子集**
第一个环节是“子集搜索”问题。给定特征集合{$a_1,a_2,...,a_d$},将每个特征看作一个候选子集，对这d个候选单特征子集进行评价，假定{$a_2$}最优，于是将{$a_2$}作为第一轮的选定集
**两个特征子集**
在上一轮的选定集中加入一个特征集，构成包含两个特征的候选子集。假定在d-1各候选两特征子集中{$a_2,a_4$}最优，且优于{$a_2$}，于是将{$a_2,a_4$}作为本轮的选定集
**停止**
假定在第k+1伦时，最优的(k+1)特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的k特征集合作为特征选择结果。
### 后向搜索backward
从完整特征集合开始，每次尝试去掉一个无关特征，逐渐减少特征。
### 双向搜索bidirectional
将前向与后向搜索结合，每一轮之间增加选定相关特征(这些特征在后续论中将不会被去除)，同时减少无关特征。
### 问题
上面的策略是贪心的，仅考虑本论选定集最优。
例如，第三轮假定选择$a_5$优于$a_6$，于是选定集为{$a_2,a_4,a_5$}，然额日在第四轮可能是{$a_2,a_4,a_6,a_8$}，比所有的{$a_2,a_4,a_5,a_i$}都更优
不进行穷举搜索，这样的问题就无法避免
## 子集评价
### 信息熵
![avatar](\信息增益.png)
![avatar](\信息熵.png)
信息增益越大，特征子集包含的有助于分类的信息越多。对每个候选特征子集，可以基于训练数据D来计算信息增益，以此作为评价标准。
### 其他方法
![avatar](\其他方法.png)
## 特征选择
![avatar](\特征选择.png)

## 常见特征选择方法
* 过滤式filter
* 包裹是wrapper
* 嵌入式embedding