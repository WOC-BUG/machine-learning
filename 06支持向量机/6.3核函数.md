# 6.3核函数

## 原理
不可分的数据映射到高维空间再分类
![avatar](\再分类.png)

## 公式
令Φ(x)表示将x映射后的特征向量，则对应模型为：
$f(x)=w^TΦ(x)+b$
![avatar](\对偶问题.png)

约束条件：
![avatar](\约束条件.png)

## 核技巧
**直接计算$Φ(x_i)^TΦ(x_j)$运算量非常大，因此运用公式：**
![avatar](\核技巧.png)

$x_i$和$x_j$在特征空间的内积等于他们在原始样本空间中通过函数$k(·,·)$计算的结果。这样就不必计算高维空间的内积。

**函数可重写为：**
![avatar](\重写函数.png)

**找到k的例子：**

![avatar](\例子.png)
假设x原本从二维映射到了三维

![avatar](\找到k.png)
得到$Φ(a)^TΦ(b)=(a^Tb)^2$

则$k(a,b)=(a^Tb)^2$

## 常用核函数
![avatar](\常用核函数.png)

和SKlearn比较对应的常用核函数，可用于调包：
![avatar](\common&#32;kernels.png)

## Polynomial Kernel多项式

### 公式
$(a*b+r)^d$，其中,a为正例，b为反例，d为degree次方数，r为coefficient

一维对应到二维：
![avatar](\Polynomial&#32;Kernel.png)

假设r=$\frac{1}{2}$，d=2
![avatar](\r=0.5d=2.png)
其中，这两个$\frac{1}{2}$是第三维上的值，它们相等，所以不考虑

### 举例
![avatar](\举例.png)
可以看出r相当于在x轴上的缩放程度


### sklearn实例：不同参数设定对比
![avatar](\原数据.png)

d=3,r=1,C=5和d=10,r=100,C=5时参数对比(其中C是容错率)：
![avatar](\不同参数对比.png)

## Radial Kernel Gassian RBF

### 公式
$e^{-γ(a-b)^2}$
当$γ=\frac{1}{2}$时，对上式进行泰勒展开得到公式：
![avatar](\泰勒展开.png)

式中$(a-b)^2$说明衡量的是a和b距离的一个函数，a和b距离越远，影响就越小，a和b距离越小影响越大。类似于邻近法。
![avatar](\ab距离影响.png)


### 推导

#### RBF变形

![avatar](\RBF变形.png)

#### 令$γ=\frac{1}{2}$
![avatar](\gamma等于0.5.png)

#### 泰勒展开公式
![avatar](\泰勒展开2.png)

#### 令a=0
![avatar](\令a=0.png)


#### 对RBF最后一项做泰勒展开
![avatar](\对RBF最后一项做泰勒展开.png)
![avatar](\r=0.png)
其中每一项都有的$(ab)^d$相当于Polynomial Kernel多项式中的r=0
![avatar](\e^ab.png)

#### RBF变形
![avatar](\RBF变形2.png)
令![avatar](\s.png)

#### 最终式子
![avatar](\最终式子.png)

#### 两个点在无穷维度的关系
![avatar](\两个点在无穷维度的关系.png)

### 调参对比
![avatar](\调参对比.png)