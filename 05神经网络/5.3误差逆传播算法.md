# 5.3误差逆传播算法

误差逆传播(error BackPropagation,简称 BP)算法，适用于多层前馈神经网络、训练递归神经网络等多种神经网络。

## BP网络及算法中的变量符号
![avatar](\BP网络.png)
其中，输出层第j个神经元的阈值为$\theta_j$，隐层第h个神经元的阈值为$\gamma_h$。

### 输出及误差
假设隐层和输出层神经元都使用Sigmoid函数：
对训练例($x_k,y_k$)，假定神经网络的输出为$\hat{y}_k=(\hat{y}_1^k,\hat{y}_2^k,...,\hat{y}_l^k)$，即$\hat{y}_k=f(\beta_j-\theta_j)$。
则网络在($x_k,y_k$)上的均方误差为$E_k=\frac{1}{2}\sum^l_{j=1}(\hat{y}^k_j-y^k_j)^2$

### 未知参数
上图一共有(d+l+1)q+l个参数待确定：
* 输入层到隐层d*q个权值
* 隐层到输出层q*l个权值
* q个隐层神经元阈值，l个输出层神经元阈值

任意参数的更新估计式均为：v←v+Δv

## 标准BP算法
**(每次仅针对一个训练样例更新连接权和阈值)**

### 例子
以$w_{hj}$为例，算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。

1. Δ$w_{hj}=-μ\frac{\partial E_k}{\partial w_{hj}}$，（$E_k$为误差，μ为学习率）
2. $\frac{\partial E_k}{\partial w_{hj}}=\frac{\partial E_k}{\partial \hat{y}_j^k}*\frac{\partial \hat{y}_j^k}{\partial β_{j}}*\frac{\partial β_{j}}{\partial w_{hj}}$，（$β_j$和$\hat{y}_j^k$为第j个输出层神经元的输入值和输出值）
3. 由$β_j$的定义，得$\frac{\partial β_{j}}{\partial w_{hj}}=b_h$
4. Digmoid函数有个很好的性质：f'(x)=f(x)(1-f(x))
5. 由输出及误差公式可得：$g_j=-\frac{\partial E_k}{\partial \hat{y}_j^k}*\frac{\partial \hat{y}_j^k}{\partial β_{j}}=-(\hat{y}^k_j-y^k_j)f'(β_j-θ_j)=-\hat{y}^k_j(1-\hat{y}^k_j)(y^k_j-\hat{y}^k_j)$
6. 将3式和5式带入2式得：$\frac{\partial E_k}{\partial w_{hj}}=-\hat{y}^k_j(1-\hat{y}^k_j)(y^k_j-\hat{y}^k_j)*b_h$
7. 最后将6式带入1式得：Δ$w_{hj}=μ\hat{y}^k_j(1-\hat{y}^k_j)(y^k_j-\hat{y}^k_j)b_h=μg_jb_h$
8. 同理可得：$Δθ_j=μg_j$，$Δv_{ih}=μe_hx_i$，$Δγ_h=-μe_h$（其中$e_h=-\frac{\partial E_k}{\partial b_h}*\frac{\partial b_h}{\partial α_h}=b_h(1-b_h)\sum^l_{j=1}w_{hj}g_j$）
学习率μ未必相等，太大容易震荡，太小会使收敛速度过慢。有时，$w_{hj}、θ_j$可使用$μ_1$，$v_{ih}、γ_h$可使用$μ_2$。

### BP算法工作流程
![avatar](\误差逆传播算法.png)

西瓜数据中网络参数和分类边界变化情况：
![avatar](\西瓜数据.png)


## 累计误差逆传播(accumulated error backpropagation)算法
**(读取整个训练集D一遍后才对参数更新)**

一般累计误差下降到一定程度后，进一步下降会很慢，这时使用标准BP会得到较好的解。

## 缓解BP网络的过拟合
### 1. 早停(early stopping)
将数据分成训练集和验证集，训练集用于计算梯度、更新连接权、阈值，验证集用来估计误差。若训练集误差降低，但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。

### 2. 正则化(regularrization)
在误差目标函数中增加一个用于描述网络复杂度的部分。例如增加连接权与阈值的平方和，则E=λ$\frac{1}{m}\sum^m_{k=1}E_k+(1+λ)\sum_iw_i^2$，其中$E_k$为第k个训练样例集上的误差，$w_i$为连接权和阈值，λ∈(0,1)用于对经验误差与网络复杂度着两项析中，常通过交叉验证法来估计。