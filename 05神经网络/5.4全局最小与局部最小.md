# 5.4全局最小与局部最小

## 概念：
若E表示神经网络在训练集上的误差，则它是关于连接权w和阈值θ的函数。此时，神经网络的训练过程可看作一个参数的寻优过程，即在参数空间中寻找一组**最优**参数，使得E最小。

## 两种“最优”
### 1. 局部极小(local minimum)
对w* 和 θ*，若存在ε>0使得
![avatar](\w和θ.png)
都有E(w;θ)≥E(w*;θ*)成立，则E(w*;θ*)为局部最小解；

* 局部最小解是参数空间中的某个点，其邻域点的误差函数值均不小于该点的函数值
### 2. 全局最小(global minimum)

若对参数空间任意(w;θ)都有E(w;θ)≥E(w*;θ*)，则E(w*;θ*)为全局最小解。

* 全局最小解是参数空间中所有点的误差函数值均不小于该点的误差函数值。

![avatar](\全局最小与局部极小.png)
图中有两个局部极小，但只有一个全局最小

### 基于梯度的搜索
从某些初始值触发，迭代寻找最优参数。每次迭代中，先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。当误差函数梯度为零时，则达到局部极小。

* 若函数只有一个局部极小，那么它就是全局最小；
* 若函数有多个局部极小，则不能保证找到的解时全局最小，称这种情况为：参数寻优陷入局部极小。


### “跳出”局部极小的策略
* **以多组不同参数值初始化多个神经网络，按照标准方法训练后，取其中误差最小的解作为最终参数。** 这相当于从多个不同的初始点开始搜索，这样就可能陷入不同的局部极小，从中进行选择又肯获得更接近全局最小的结果。
* **使用“模拟退火”(simulated annealing)技术。** 模拟退火在每一步都以一定的概率接受比当前更差的结果，从而有助于“跳出”局部极小。在每步迭代过程中，接受“次优解”的概率要随着时间推移逐渐降低，从而保证稳定算法。
* **使用随机梯度下降。** 与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机因素。于是，即便陷入局部极小点，它计算出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索。