# 8.3Bagging与随机森林
## 目的
保证“好而不同”的不同

## 思路
从训练集中进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合，产生最终的预测结果。

## 图示
![avatar](\图示.png)

## 分类：取样方法
* Bagging：放回抽样
* Pasting：不放回抽样
* bootstrap_feature：在特征空间随机采样
* 既针对样本，又针对特征进行随机采样

## 8.3.1 Bagging
基于自主采样法抽样

### 基本流程
给定包含m个样本的数据集，m次放回抽样，得到m个样本的采样集，初始训练集中约有63.2%的样本出现在采样集中。照这样，空可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。

### 复杂度
假设基学习器的计算复杂度为O(m)，则Bagging复杂度大致为T(O(m)+O(s))。

### 包外估计
由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用作验证集来对泛化性能进行包外估计(out-of-bag estimate)。
为此需记录每个基学习器所使用的训练样本。
![avatar](\Hoob.png)
（$D_t$表示$h_t$实际使用的训练样本集，$H^{oob}(x)表示对样本x的包外预测$）

### Bagging泛化误差的包外估计为

### ![avatar](\包外估计.png)


代码例子：
https://www.bilibili.com/video/av79015715?p=117


## 8.3.2 随机森林

### 思路
* 样本和节点判断的特征都随机选
* 既添加样本扰动又添加属性扰动

### 对比Bagging与随机森林
![avatar](\对比随机森林和Bagging.png)
当分类器数量较少时，随机森林表现不好
当分类器数量增加时，随机森林的泛化性能优于Bagging

### 补充：Extra-Tress
#### 原理
特征随机，样本不随机

#### ET与RF的区别
1. RF应用了Bagging进行随机抽样，而ET的每棵决策树应用的时相同的样本
2. RF在一个随机子集内基于信息熵和基尼指数寻找最优属性，而ET完全随机寻找一个特征值进行划分