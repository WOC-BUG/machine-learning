# 8.2Boosting
## 思路
集成多个模型，每个模型都在尝试增强整体效果。
具体来说：就是先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先学习器做的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个学习器，如此重复进行，直至学习器数目达到预先指定的值。

## 图示
![avatar](\Boosting.png)
如图，经过第一个学习器的学习后给预测错误样本呈现为更深的蓝色，训练正确的为浅蓝色的点，赋予错误的训练样本以跟高的权重，对于经过调整的训练样本再次训练下一个学习器，同理一直迭代下去。

## 形式
![avatar](\集成学习器的形式.png)

## 例
![avatar](\例子.png)
x：输入
y：输出，1为正例，-1为反例
v：阈值（判定正反例的分界线）

### 第一个学习器
假设每个$x_i$的权重一致为0.1(权重用于计算误差)
![avatar](\D1.png)
**D1**

* 阈值v=2.5时分类误差率最低（x=6,7,8错分为反例，误差为它们的权重之和$e_1=0.1+0.1+0.1=0.3$），故个体学习器为：
$$ G_1(x)\left\{
\begin{aligned}
\text{1,x<2.5} \\
\text{-1,x>2.5}
\end{aligned}
\right.
$$
* 根据误差$e_1$计算系数$α_1=\frac{1}{2}log\frac{1-e_1}{e_1}=0.4236$(只有$e_i<\frac{1}{2}时，α_1>0$)
* 跟新训练数据的权值分布
  ![avatar](\更新训练数据的权值分布.png)
  （通过指数损失函数exp(x)调整权重，$y_i>0$则$-α_iy_iG_m(x_i)<0$，反之$-α_iy_iG_m(x_i)>0$，即分类正确的降低权重，分类错误的增加权重）
![avatar](\signf1(x).png)

### 第二个学习器
![avatar](\D2.png)
**D2**

* 阈值v=8.5时分类误差率最低（x=3,4,5错分为正例，误差为它们的权重之和$e_2=0.07143+0.07143+0.07143=0.2143$），故个体学习器为：
$$ G_2(x)\left\{
\begin{aligned}
\text{1,x<8.5} \\
\text{-1,x>8.5}
\end{aligned}
\right.
$$
* 根据误差$e_1$计算系数$α_2=\frac{1}{2}log\frac{1-e_2}{e_2}=0.6496$
* 跟新训练数据的权值分布
  ![avatar](\更新训练数据的权值分布2.png)
  ![avatar](\signf2(x).png)


### 第三个学习器
![avatar](\D3.png)
**D2**

* 阈值v=5.5时分类误差率最低（x=3,4,5错分为正例，误差为它们的权重之和$e_2=0.1820$），故个体学习器为：
$$ G_3(x)\left\{
\begin{aligned}
\text{1,x<5.5} \\
\text{-1,x>5.5}
\end{aligned}
\right.
$$
* 根据误差$e_1$计算系数$α_3=\frac{1}{2}log\frac{1-e_3}{e_3}=0.7514$
* 跟新训练数据的权值分布
  ![avatar](\更新训练数据的权值分布3.png)
  ![avatar](\signf3(x).png)

### 最终结果
![avatar](\最终结果.png)